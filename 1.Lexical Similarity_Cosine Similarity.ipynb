{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity : Spam/non-spam dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset \"SMS SpamCollection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>- Rodger Burns - MSG = We tried to call you r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. But not conform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;DECIMAL&amp;gt; m but its not a common car he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>, Do you want a New Nokia i colour phone Deli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0  spam   - Rodger Burns - MSG = We tried to call you r...\n",
       "1   ham                &lt;#&gt;  in mca. But not conform.\n",
       "2   ham   &lt;#&gt;  mins but i had to stop somewhere f...\n",
       "3   ham   &lt;DECIMAL&gt; m but its not a common car he...\n",
       "4  spam   , Do you want a New Nokia i colour phone Deli..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Spam_data = pd.read_csv(\"SMS SpamCollection.csv\")\n",
    "\n",
    "Spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>- Rodger Burns - MSG = We tried to call you r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. But not conform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;DECIMAL&amp;gt; m but its not a common car he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>, Do you want a New Nokia i colour phone Deli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0  spam   - Rodger Burns - MSG = We tried to call you r...\n",
       "1   ham                &lt;#&gt;  in mca. But not conform.\n",
       "2   ham   &lt;#&gt;  mins but i had to stop somewhere f...\n",
       "3   ham   &lt;DECIMAL&gt; m but its not a common car he...\n",
       "4  spam   , Do you want a New Nokia i colour phone Deli..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Spam_df = Spam_data[['label','message']]\n",
    "Spam_df[\"message\"] = Spam_df[\"message\"].astype(str)\n",
    "Spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. Data Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing was done to remove the noise present in data. \n",
    "### Pre-processing steps followed :\n",
    "### i.    Conversion of the characters into lower case\n",
    "### ii.   Removal of Punctuation\n",
    "### iii.  Removal of Stopwords\n",
    "### iv.  Removal of frequently occuring words\n",
    "### v.   Removal of rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Conversion of the characters into lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way.\n",
    "\n",
    "This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        - rodger burns - msg = we tried to call you r...\n",
       "1                     &lt;#&gt;  in mca. but not conform.\n",
       "2        &lt;#&gt;  mins but i had to stop somewhere f...\n",
       "3        &lt;decimal&gt; m but its not a common car he...\n",
       "4        , do you want a new nokia i colour phone deli...\n",
       "                              ...                        \n",
       "5568                              yup... how 端 noe leh...\n",
       "5569    yup... i havent been there before... you want ...\n",
       "5570    yup... ok i go home look at the timings then i...\n",
       "5571    yupz... i've oredi booked slots  my weekends l...\n",
       "5572    zoe it just hit me  im fucking shitin myself i...\n",
       "Name: message_lower, Length: 5573, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Spam_df[\"message_lower\"] = Spam_df[\"message\"].str.lower()\n",
    "Spam_df[\"message_lower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>message_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>- Rodger Burns - MSG = We tried to call you r...</td>\n",
       "      <td>- rodger burns - msg = we tried to call you r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. But not conform.</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. but not conform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;DECIMAL&amp;gt; m but its not a common car he...</td>\n",
       "      <td>&amp;lt;decimal&amp;gt; m but its not a common car he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>, Do you want a New Nokia i colour phone Deli...</td>\n",
       "      <td>, do you want a new nokia i colour phone deli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  \\\n",
       "0  spam   - Rodger Burns - MSG = We tried to call you r...   \n",
       "1   ham                &lt;#&gt;  in mca. But not conform.   \n",
       "2   ham   &lt;#&gt;  mins but i had to stop somewhere f...   \n",
       "3   ham   &lt;DECIMAL&gt; m but its not a common car he...   \n",
       "4  spam   , Do you want a New Nokia i colour phone Deli...   \n",
       "\n",
       "                                       message_lower  \n",
       "0   - rodger burns - msg = we tried to call you r...  \n",
       "1                &lt;#&gt;  in mca. but not conform.  \n",
       "2   &lt;#&gt;  mins but i had to stop somewhere f...  \n",
       "3   &lt;decimal&gt; m but its not a common car he...  \n",
       "4   , do you want a new nokia i colour phone deli...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. Removal of Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common text preprocessing technique is to remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n",
    "\n",
    "It is important to choose the list of punctuations to exclude depending on the use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>message_lower</th>\n",
       "      <th>message_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>- Rodger Burns - MSG = We tried to call you r...</td>\n",
       "      <td>- rodger burns - msg = we tried to call you r...</td>\n",
       "      <td>rodger burns  msg  we tried to call you re y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. But not conform.</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. but not conform.</td>\n",
       "      <td>ltgt  in mca but not conform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "      <td>ltgt  mins but i had to stop somewhere first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;DECIMAL&amp;gt; m but its not a common car he...</td>\n",
       "      <td>&amp;lt;decimal&amp;gt; m but its not a common car he...</td>\n",
       "      <td>ltdecimalgt m but its not a common car here s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>, Do you want a New Nokia i colour phone Deli...</td>\n",
       "      <td>, do you want a new nokia i colour phone deli...</td>\n",
       "      <td>do you want a new nokia i colour phone deliv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  \\\n",
       "0  spam   - Rodger Burns - MSG = We tried to call you r...   \n",
       "1   ham                &lt;#&gt;  in mca. But not conform.   \n",
       "2   ham   &lt;#&gt;  mins but i had to stop somewhere f...   \n",
       "3   ham   &lt;DECIMAL&gt; m but its not a common car he...   \n",
       "4  spam   , Do you want a New Nokia i colour phone Deli...   \n",
       "\n",
       "                                       message_lower  \\\n",
       "0   - rodger burns - msg = we tried to call you r...   \n",
       "1                &lt;#&gt;  in mca. but not conform.   \n",
       "2   &lt;#&gt;  mins but i had to stop somewhere f...   \n",
       "3   &lt;decimal&gt; m but its not a common car he...   \n",
       "4   , do you want a new nokia i colour phone deli...   \n",
       "\n",
       "                                 message_punctuation  \n",
       "0    rodger burns  msg  we tried to call you re y...  \n",
       "1                       ltgt  in mca but not conform  \n",
       "2       ltgt  mins but i had to stop somewhere first  \n",
       "3   ltdecimalgt m but its not a common car here s...  \n",
       "4    do you want a new nokia i colour phone deliv...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### remove Punctuation\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "Spam_df[\"message_punctuation\"] = Spam_df[\"message_lower\"].apply(lambda text: remove_punctuation(text))\n",
    "Spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii. Removal of stopwords\n",
    "\n",
    "Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\n",
    "\n",
    "These stopword lists are already compiled for different languages and we can safely use them. \n",
    "\n",
    "### For example, the stopword list for english language from the nltk package can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>message_lower</th>\n",
       "      <th>message_punctuation</th>\n",
       "      <th>message_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>- Rodger Burns - MSG = We tried to call you r...</td>\n",
       "      <td>- rodger burns - msg = we tried to call you r...</td>\n",
       "      <td>rodger burns  msg  we tried to call you re y...</td>\n",
       "      <td>rodger burns msg tried call reply sms free nok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. But not conform.</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. but not conform.</td>\n",
       "      <td>ltgt  in mca but not conform</td>\n",
       "      <td>ltgt mca conform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "      <td>ltgt  mins but i had to stop somewhere first</td>\n",
       "      <td>ltgt mins stop somewhere first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;DECIMAL&amp;gt; m but its not a common car he...</td>\n",
       "      <td>&amp;lt;decimal&amp;gt; m but its not a common car he...</td>\n",
       "      <td>ltdecimalgt m but its not a common car here s...</td>\n",
       "      <td>ltdecimalgt common car better buy china asia f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>, Do you want a New Nokia i colour phone Deli...</td>\n",
       "      <td>, do you want a new nokia i colour phone deli...</td>\n",
       "      <td>do you want a new nokia i colour phone deliv...</td>\n",
       "      <td>want new nokia colour phone deliveredtomorrow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  \\\n",
       "0  spam   - Rodger Burns - MSG = We tried to call you r...   \n",
       "1   ham                &lt;#&gt;  in mca. But not conform.   \n",
       "2   ham   &lt;#&gt;  mins but i had to stop somewhere f...   \n",
       "3   ham   &lt;DECIMAL&gt; m but its not a common car he...   \n",
       "4  spam   , Do you want a New Nokia i colour phone Deli...   \n",
       "\n",
       "                                       message_lower  \\\n",
       "0   - rodger burns - msg = we tried to call you r...   \n",
       "1                &lt;#&gt;  in mca. but not conform.   \n",
       "2   &lt;#&gt;  mins but i had to stop somewhere f...   \n",
       "3   &lt;decimal&gt; m but its not a common car he...   \n",
       "4   , do you want a new nokia i colour phone deli...   \n",
       "\n",
       "                                 message_punctuation  \\\n",
       "0    rodger burns  msg  we tried to call you re y...   \n",
       "1                       ltgt  in mca but not conform   \n",
       "2       ltgt  mins but i had to stop somewhere first   \n",
       "3   ltdecimalgt m but its not a common car here s...   \n",
       "4    do you want a new nokia i colour phone deliv...   \n",
       "\n",
       "                                   message_stopwords  \n",
       "0  rodger burns msg tried call reply sms free nok...  \n",
       "1                                   ltgt mca conform  \n",
       "2                     ltgt mins stop somewhere first  \n",
       "3  ltdecimalgt common car better buy china asia f...  \n",
       "4  want new nokia colour phone deliveredtomorrow ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### remove stopwords\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "Spam_df[\"message_stopwords\"] = Spam_df[\"message_punctuation\"].apply(lambda text: remove_stopwords(text))\n",
    "Spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iv. Removal of Frequent words\n",
    "\n",
    "In the previos preprocessing step, we removed the stopwords based on language information. But cases, where we have a domain specific corpus, we might also have some frequent words which are of not so much importance to us.\n",
    "\n",
    "So this step is to remove the frequent words in the given corpus. \n",
    "Let us get the most common words adn then remove them in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('u', 1156),\n",
       " ('call', 580),\n",
       " ('im', 466),\n",
       " ('get', 390),\n",
       " ('ur', 390),\n",
       " ('dont', 287),\n",
       " ('go', 285),\n",
       " ('free', 278),\n",
       " ('ok', 278),\n",
       " ('ltgt', 276)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Frequently occuring words\n",
    "\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in Spam_df[\"message_stopwords\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>message_lower</th>\n",
       "      <th>message_punctuation</th>\n",
       "      <th>message_stopwords</th>\n",
       "      <th>message_stopwords_frequent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>- Rodger Burns - MSG = We tried to call you r...</td>\n",
       "      <td>- rodger burns - msg = we tried to call you r...</td>\n",
       "      <td>rodger burns  msg  we tried to call you re y...</td>\n",
       "      <td>rodger burns msg tried call reply sms free nok...</td>\n",
       "      <td>rodger burns msg tried reply sms nokia mobile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. But not conform.</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. but not conform.</td>\n",
       "      <td>ltgt  in mca but not conform</td>\n",
       "      <td>ltgt mca conform</td>\n",
       "      <td>mca conform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "      <td>ltgt  mins but i had to stop somewhere first</td>\n",
       "      <td>ltgt mins stop somewhere first</td>\n",
       "      <td>mins stop somewhere first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;DECIMAL&amp;gt; m but its not a common car he...</td>\n",
       "      <td>&amp;lt;decimal&amp;gt; m but its not a common car he...</td>\n",
       "      <td>ltdecimalgt m but its not a common car here s...</td>\n",
       "      <td>ltdecimalgt common car better buy china asia f...</td>\n",
       "      <td>ltdecimalgt common car better buy china asia f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>, Do you want a New Nokia i colour phone Deli...</td>\n",
       "      <td>, do you want a new nokia i colour phone deli...</td>\n",
       "      <td>do you want a new nokia i colour phone deliv...</td>\n",
       "      <td>want new nokia colour phone deliveredtomorrow ...</td>\n",
       "      <td>want new nokia colour phone deliveredtomorrow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  \\\n",
       "0  spam   - Rodger Burns - MSG = We tried to call you r...   \n",
       "1   ham                &lt;#&gt;  in mca. But not conform.   \n",
       "2   ham   &lt;#&gt;  mins but i had to stop somewhere f...   \n",
       "3   ham   &lt;DECIMAL&gt; m but its not a common car he...   \n",
       "4  spam   , Do you want a New Nokia i colour phone Deli...   \n",
       "\n",
       "                                       message_lower  \\\n",
       "0   - rodger burns - msg = we tried to call you r...   \n",
       "1                &lt;#&gt;  in mca. but not conform.   \n",
       "2   &lt;#&gt;  mins but i had to stop somewhere f...   \n",
       "3   &lt;decimal&gt; m but its not a common car he...   \n",
       "4   , do you want a new nokia i colour phone deli...   \n",
       "\n",
       "                                 message_punctuation  \\\n",
       "0    rodger burns  msg  we tried to call you re y...   \n",
       "1                       ltgt  in mca but not conform   \n",
       "2       ltgt  mins but i had to stop somewhere first   \n",
       "3   ltdecimalgt m but its not a common car here s...   \n",
       "4    do you want a new nokia i colour phone deliv...   \n",
       "\n",
       "                                   message_stopwords  \\\n",
       "0  rodger burns msg tried call reply sms free nok...   \n",
       "1                                   ltgt mca conform   \n",
       "2                     ltgt mins stop somewhere first   \n",
       "3  ltdecimalgt common car better buy china asia f...   \n",
       "4  want new nokia colour phone deliveredtomorrow ...   \n",
       "\n",
       "                          message_stopwords_frequent  \n",
       "0  rodger burns msg tried reply sms nokia mobile ...  \n",
       "1                                        mca conform  \n",
       "2                          mins stop somewhere first  \n",
       "3  ltdecimalgt common car better buy china asia f...  \n",
       "4  want new nokia colour phone deliveredtomorrow ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "Spam_df[\"message_stopwords_frequent\"] = Spam_df[\"message_stopwords\"].apply(lambda text: remove_freqwords(text))\n",
    "Spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v. Removal of Rare words\n",
    "\n",
    "This is very similar to previous preprocessing step but we will remove the rare words from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>message_lower</th>\n",
       "      <th>message_punctuation</th>\n",
       "      <th>message_stopwords</th>\n",
       "      <th>message_stopwords_frequent</th>\n",
       "      <th>message_stopwords_rare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>- Rodger Burns - MSG = We tried to call you r...</td>\n",
       "      <td>- rodger burns - msg = we tried to call you r...</td>\n",
       "      <td>rodger burns  msg  we tried to call you re y...</td>\n",
       "      <td>rodger burns msg tried call reply sms free nok...</td>\n",
       "      <td>rodger burns msg tried reply sms nokia mobile ...</td>\n",
       "      <td>rodger burns msg tried reply sms nokia mobile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. But not conform.</td>\n",
       "      <td>&amp;lt;#&amp;gt;  in mca. but not conform.</td>\n",
       "      <td>ltgt  in mca but not conform</td>\n",
       "      <td>ltgt mca conform</td>\n",
       "      <td>mca conform</td>\n",
       "      <td>mca conform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "      <td>&amp;lt;#&amp;gt;  mins but i had to stop somewhere f...</td>\n",
       "      <td>ltgt  mins but i had to stop somewhere first</td>\n",
       "      <td>ltgt mins stop somewhere first</td>\n",
       "      <td>mins stop somewhere first</td>\n",
       "      <td>mins stop somewhere first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;DECIMAL&amp;gt; m but its not a common car he...</td>\n",
       "      <td>&amp;lt;decimal&amp;gt; m but its not a common car he...</td>\n",
       "      <td>ltdecimalgt m but its not a common car here s...</td>\n",
       "      <td>ltdecimalgt common car better buy china asia f...</td>\n",
       "      <td>ltdecimalgt common car better buy china asia f...</td>\n",
       "      <td>ltdecimalgt common car better buy china asia f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>, Do you want a New Nokia i colour phone Deli...</td>\n",
       "      <td>, do you want a new nokia i colour phone deli...</td>\n",
       "      <td>do you want a new nokia i colour phone deliv...</td>\n",
       "      <td>want new nokia colour phone deliveredtomorrow ...</td>\n",
       "      <td>want new nokia colour phone deliveredtomorrow ...</td>\n",
       "      <td>want new nokia colour phone deliveredtomorrow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  \\\n",
       "0  spam   - Rodger Burns - MSG = We tried to call you r...   \n",
       "1   ham                &lt;#&gt;  in mca. But not conform.   \n",
       "2   ham   &lt;#&gt;  mins but i had to stop somewhere f...   \n",
       "3   ham   &lt;DECIMAL&gt; m but its not a common car he...   \n",
       "4  spam   , Do you want a New Nokia i colour phone Deli...   \n",
       "\n",
       "                                       message_lower  \\\n",
       "0   - rodger burns - msg = we tried to call you r...   \n",
       "1                &lt;#&gt;  in mca. but not conform.   \n",
       "2   &lt;#&gt;  mins but i had to stop somewhere f...   \n",
       "3   &lt;decimal&gt; m but its not a common car he...   \n",
       "4   , do you want a new nokia i colour phone deli...   \n",
       "\n",
       "                                 message_punctuation  \\\n",
       "0    rodger burns  msg  we tried to call you re y...   \n",
       "1                       ltgt  in mca but not conform   \n",
       "2       ltgt  mins but i had to stop somewhere first   \n",
       "3   ltdecimalgt m but its not a common car here s...   \n",
       "4    do you want a new nokia i colour phone deliv...   \n",
       "\n",
       "                                   message_stopwords  \\\n",
       "0  rodger burns msg tried call reply sms free nok...   \n",
       "1                                   ltgt mca conform   \n",
       "2                     ltgt mins stop somewhere first   \n",
       "3  ltdecimalgt common car better buy china asia f...   \n",
       "4  want new nokia colour phone deliveredtomorrow ...   \n",
       "\n",
       "                          message_stopwords_frequent  \\\n",
       "0  rodger burns msg tried reply sms nokia mobile ...   \n",
       "1                                        mca conform   \n",
       "2                          mins stop somewhere first   \n",
       "3  ltdecimalgt common car better buy china asia f...   \n",
       "4  want new nokia colour phone deliveredtomorrow ...   \n",
       "\n",
       "                              message_stopwords_rare  \n",
       "0  rodger burns msg tried reply sms nokia mobile ...  \n",
       "1                                        mca conform  \n",
       "2                          mins stop somewhere first  \n",
       "3  ltdecimalgt common car better buy china asia f...  \n",
       "4  want new nokia colour phone deliveredtomorrow ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####  Rare words\n",
    "\n",
    "n_rare_words = 10\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "Spam_df[\"message_stopwords_rare\"] = Spam_df[\"message_stopwords_frequent\"].apply(lambda text: remove_rarewords(text))\n",
    "Spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns which are no more needed \n",
    "\n",
    "Spam_df.drop([\"message\",\"message_lower\", \"message_punctuation\",\"message_stopwords\",\"message_stopwords_frequent\"], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message_stopwords_rare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>rodger burns msg tried reply sms nokia mobile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>mca conform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>mins stop somewhere first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>ltdecimalgt common car better buy china asia f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>want new nokia colour phone deliveredtomorrow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                             message_stopwords_rare\n",
       "0  spam  rodger burns msg tried reply sms nokia mobile ...\n",
       "1   ham                                        mca conform\n",
       "2   ham                          mins stop somewhere first\n",
       "3   ham  ltdecimalgt common car better buy china asia f...\n",
       "4  spam  want new nokia colour phone deliveredtomorrow ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping only the final column\n",
    "\n",
    "Spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the final column\n",
    "\n",
    "Spam_df.rename(columns = {'message_stopwords_rare':'messages'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>rodger burns msg tried reply sms nokia mobile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>mca conform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>mins stop somewhere first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>ltdecimalgt common car better buy china asia f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>want new nokia colour phone deliveredtomorrow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           messages\n",
       "0  spam  rodger burns msg tried reply sms nokia mobile ...\n",
       "1   ham                                        mca conform\n",
       "2   ham                          mins stop somewhere first\n",
       "3   ham  ltdecimalgt common car better buy china asia f...\n",
       "4  spam  want new nokia colour phone deliveredtomorrow ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the following models :\n",
    "\n",
    "### b. Bernoulli Na誰ve Bayes using bigram tf-idf tokenization\n",
    "\n",
    "### c. Bernoulli Na誰ve Bayes using bigram BOW(Bag of words) tokenization\n",
    "\n",
    "### d. Multinomial Na誰ve Bayes using trigram BOW tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating DTM (Document term matrix) using CountVectorizer module of scikit-learn.\n",
    "\n",
    "#### tokenizer = Overrides the string tokenization step, we generatre tokenizer from NLTK's Regex tokenizer (by default: None)\n",
    "#### lowercase = True (no need to use, as it is set True by default)\n",
    "#### stop_words = 'english' (by default None is used, to improve the result we can provide custom made list of stop words)\n",
    "#### ngram_range = (1,1) (by defualt its (1,1) i.e strictly monograms will be used, (2,2) only bigrams while (1,2) uses both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(Spam_df['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, Spam_df['label'], test_size=0.25, random_state=5)\n"
   ]
  },
  {
   "attachments": {
    "1_tjcmj9cDQ-rHXAtxCu5bRQ.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm\n",
    "\n",
    "A Naive Bayes classifier is a probabilistic machine learning model thats used for classification task. The crux of the classifier is based on the Bayes theorem.\n",
    "\n",
    "Bayes Theorem:\n",
    "![1_tjcmj9cDQ-rHXAtxCu5bRQ.png](attachment:1_tjcmj9cDQ-rHXAtxCu5bRQ.png)\n",
    "\n",
    "Using Bayes theorem, we can find the probability of A happening, given that B has occurred. Here, B is the evidence and A is the hypothesis. The assumption made here is that the predictors/features are independent. That is presence of one particular feature does not affect the other. Hence it is called naive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Naive Bayes Classifier:\n",
    "\n",
    "### i. Multinomial Naive Bayes:\n",
    "This is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics, technology etc. The features/predictors used by the classifier are the frequency of the words present in the document.\n",
    "### ii. Bernoulli Naive Bayes:\n",
    "This is similar to the multinomial naive bayes but the predictors are boolean variables. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not.\n",
    "### iii. Gaussian Naive Bayes:\n",
    "When the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For our dataset, we have used \n",
    "### Multinomial Naive Bayes : MNB\n",
    "### Bernoulli Naive Bayes  : BNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes : MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predicted = MNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.41%\n"
     ]
    }
   ],
   "source": [
    "print(str('{:04.2f}'.format(accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes : BNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB accuracy = 95.84%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "accuracy_score_bnb = metrics.accuracy_score(BNB.predict(X_test),Y_test)\n",
    "print('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. TF-IDF: Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF : Term frequency \n",
    "\n",
    "TF: Term Frequency, measures how frequently a term occurs in a document.\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "DF: Document Frequency, measures how frequently a term occurs in all documents.\n",
    "\n",
    "DF(t) = Number of documents with term t in it / Total number of documents\n",
    "\n",
    "IDF: Inverse Document Frequency, measures how important a term is in a corpus\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score_mnb = 95.91%\n",
      "accuracy_score_bnb = 96.05%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "text_count_2 = tfidf.fit_transform(Spam_df['messages'])\n",
    "\n",
    "#splitting the data in test and training\n",
    "#from sklearn.model_selection() import train_test_split()\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_count_2, Spam_df['label'],test_size=0.25,random_state=5)\n",
    "\n",
    "#defining the model\n",
    "#compilimg the model -> we are going to use already used models GNB, MNB, CNB, BNB\n",
    "#fitting the model\n",
    "MNB.fit(x_train, y_train)\n",
    "accuracy_score_mnb = metrics.accuracy_score(MNB.predict(x_test), y_test)\n",
    "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
    "\n",
    "BNB.fit(x_train, y_train)\n",
    "accuracy_score_bnb = metrics.accuracy_score(BNB.predict(x_test), y_test)\n",
    "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Bernoulli Na誰ve Bayes using bigram BOW(Bag of words) tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.92%\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.feature_extraction import CountVectorizer\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "#token = RegexpTokenizer(r'[A-Za-z0-9]+')\n",
    "cv = CountVectorizer(stop_words='english', ngram_range = (2,2), tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(Spam_df['messages'])\n",
    "\n",
    "#from sklearn.model_selection import train_test_split()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, Spam_df['label'],test_size=0.25, random_state=5)\n",
    "\n",
    "#Defining the model-> we will use MultinomialNB\n",
    "\n",
    "#Compiling the model -> We will import precompiled MNB from sklearn library\n",
    "#from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "#Fitting the model\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "\n",
    "#Evaulating the model\n",
    "#form sklearn import metrics\n",
    "accuracy_score = metrics.accuracy_score(MNB.predict(X_test), Y_test)\n",
    "print(str('{:04.2f}'.format(accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Multinomial Na誰ve Bayes using trigram BOW tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.54%\n"
     ]
    }
   ],
   "source": [
    "#It shows only a marginal imporvement, let us try with trigram tokenization now:\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', ngram_range = (3,3), tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(Spam_df['messages'])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, Spam_df['label'],test_size=0.25, random_state=5)\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "accuracy_score = metrics.accuracy_score(MNB.predict(X_test), Y_test)\n",
    "print(str('{:04.2f}'.format(accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf MNB : 95.91%\n",
    "\n",
    "### tf-idf BNB : 96.05%\n",
    "\n",
    "### bigram tokenization : 80.92%\n",
    "\n",
    "### trigram tokenization : 76.54%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli Naive Bayes tf-idf tokenization has the highest accuracy amongst all the models, with the accuracy being 96.05%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cosine Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Gensim in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Gensim) (1.15.0)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Gensim) (0.29.14)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Gensim) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Gensim) (1.18.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Gensim) (1.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from smart-open>=1.8.1->Gensim) (2.23.0)\n",
      "Requirement already satisfied: boto in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from smart-open>=1.8.1->Gensim) (2.49.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from smart-open>=1.8.1->Gensim) (1.14.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->smart-open>=1.8.1->Gensim) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->smart-open>=1.8.1->Gensim) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->smart-open>=1.8.1->Gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->smart-open>=1.8.1->Gensim) (2.9)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.16 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3->smart-open>=1.8.1->Gensim) (1.17.16)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3->smart-open>=1.8.1->Gensim) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3->smart-open>=1.8.1->Gensim) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from botocore<1.18.0,>=1.17.16->boto3->smart-open>=1.8.1->Gensim) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from botocore<1.18.0,>=1.17.16->boto3->smart-open>=1.8.1->Gensim) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk>=3.1->textblob) (0.15.1)\n",
      "Requirement already satisfied: regex in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk>=3.1->textblob) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk>=3.1->textblob) (4.46.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\roma\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considering the first 200 preprocessed messages from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the first 200 rows\n",
    "\n",
    "Data = Spam_data.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter out the \"message\" column\n",
    "\n",
    "Data = Data['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     - Rodger Burns - MSG = We tried to call you r...\n",
       "1                  &lt;#&gt;  in mca. But not conform.\n",
       "2     &lt;#&gt;  mins but i had to stop somewhere f...\n",
       "3     &lt;DECIMAL&gt; m but its not a common car he...\n",
       "4     , Do you want a New Nokia i colour phone Deli...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion into List\n",
    "\n",
    "tolist() : Returns a new List of string instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append the entire list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [] \n",
    "for i in Data: \n",
    "    if i not in res: \n",
    "        res.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list after removing duplicates : [' - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now  for delivery tomorrow', ' &lt;#&gt;  in mca. But not conform.', ' &lt;#&gt;  mins but i had to stop somewhere first.', ' &lt;DECIMAL&gt; m but its not a common car here so its better to buy from china or asia. Or if i find it less expensive. I.ll holla', ' , Do you want a New Nokia i colour phone DeliveredTomorrow? With  free minutes to any mobile +  free texts + Free Camcorder reply or call .', ' =贈 UK Break AccommodationVouchers terms & conditions apply.  claim you mustprovide your claim number which is  ', ' and  are premium phone services call ', ' and  picking them up from various points', ' and half years i missed your friendship:-)', ' at esplanade.. Do 端 mind giving me a lift cos i got no car today..', \" at the latest, g's still there if you can scrounge up some ammo and want to give the new ak a try\", ' between am-pm Cost p', ' came to look at the flat, seems ok, in his s? * Is away alot wiv work. Got woman coming at . too.', ' celebrate my b\\x92day, y else?', ' dating service cal;l  boxskch', ' days to Euro kickoff! U will be kept informed of all the latest news and results daily. Unsubscribe send GET EURO STOP to .', ' days to kick off! For Euro U will be kept up to date with the latest news and results daily. To be removed send GET TXT STOP to ', ' FREE for st week! No Nokia tone  ur mob every week just txt NOKIA to  Get txting and tell ur mates www.getzed.co.uk POBox  W WQ norm p/tone +', ' FREE TAROT TEXTS! Find out about your love life now! TRY  FOR FREE! Text CHANCE to   only! After  Free, Msgs 贈. each', \" free text msgs. Just text ok to  and we'll credit your account\", ' Free Top Polyphonic Tones call , National Rate. Get a toppoly tune sent every week, just text SUBPOLY to , 贈 per pole. UnSub .', ' FREE>Ringtone!Reply REAL', ' FROM  LOST POUND HELP', \" gonna let me know cos comes bak from holiday that day.  is coming. Don'tgettext me  number. \", \" I don't have her number and  its gonna be a massive pain in the ass and i'd rather not get involved if that's possible\", ' in cbe.  in chennai.', ' laptop... I noe infra but too slow lar... I wan fast one', ' lor... Change  suntec... Wat time u coming?', ' min later k...', ' New Mobiles from , MUST GO! Txt: NOKIA to No:  & collect yours today!From ONLY 贈 www.-tc.biz optout .gbp/mtmsg', ' New Mobiles from , MUST GO! Txt: NOKIA to No:  & collect yours today!From ONLY 贈 www.-tc.biz optout .gbp/mtmsg TXTAUCTION', ' nights...We nt staying at port step liao...Too ex', ' oclock at mine. Just to bash out a flat plan.', \" p  Alfie Moon's Children in Need song on ur mob. Tell ur ms. Txt TONE CHARITY to  for nokias or POLY CHARITY for polys :zed  profit  charity \", ' pa but not selected.', \" said kiss, kiss, i can't do the sound effects! He is a gorgeous man isn't he! Kind of person who needs a smile to brighten his day! \", \" says that he's quitting at leasttimes a day so i wudn't take much notice of that. Nah, she didn't mind. Are you gonna see him again? Do you want to come to taunton tonight? U can tell me all about !\", ' tacos +  rajas burrito, right?', ' URGENT! Your mobile No xxxxxxxxx won a 贈, bonus caller prize on //! this is the nd attempt to reach YOU! call  ASAP!', ' what number do u live at? Is it ?', ' wonders in My WORLD th You th Ur style th Ur smile th Ur Personality rd Ur Nature nd Ur SMS and st \"Ur Lovely Friendship\"... good morning dear', ' XMAS iscoming & ur awarded either 贈 CD gift vouchers & free entry  r 贈 weekly draw txt MUSIC to  TnC', ' You have won a 贈, price! To claim, call .', '&lt;#&gt;  am I think? Should say on syllabus', '&lt;#&gt;  great loxahatchee xmas tree burning update: you can totally see stars here', \"&lt;#&gt;  is fast approaching. So, Wish u a very Happy New Year Happy Sankranti Happy republic day Happy Valentines Day Happy Shivratri Happy Ugadi Happy Fools day Happy May Day Happy Independence Day, Happy Friendship,Mother,Father,Teachers,Childrens Day, &amp; HAPPY BIRTHDAY  U. Happy Ganesh festival Happy Dasara Happy Diwali Happy Christmas  &lt;#&gt;  Good Mornings Afternoons, Evenings Nights. RememberI AM the first to WISHING U ALL THESE...your's Raj\", '&lt;#&gt;  w jetton ave if you forgot', '&lt;#&gt; %of pple marry with their lovers... becz they hav gud undrstndng dat avoids problems. i sent dis  u, u wil get gud news on friday by d person you like. And tomorrow will be the best day of your life. Dont break this chain. If you break you will suffer. send this to  &lt;#&gt;  frnds in &lt;#&gt;  mins whn u read...', \"&lt;#&gt; , that's all? Guess that's easy enough\", '&lt;#&gt; ISH MINUTES WAS  MINUTES AGO. WTF.', '(And my man carlos is definitely coming by mu tonight, no excuses)', '(Bank of Granite issues Strong-Buy) EXPLOSIVE PICK FOR OUR MEMBERS *****UP OVER % *********** Nasdaq Symbol CDGT That is a $. per..', \"(I should add that I don't really care and if you can't I can at least get this dude to fuck off but hey, your money if you want it)\", \"(No promises on when though, haven't even gotten dinner yet)\", '(That said can you text him one more time?)', \"(You didn't hear it from me)\", ') Go to write msg ) Put on Dictionary mode )Cover the screen with hand, )Press  &lt;#&gt; . )Gently remove Ur hand.. Its interesting..:)', \"* Am on a train back from northampton so i'm afraid not!\", '* Am on my way', '* FREE* POLYPHONIC RINGTONE Text SUPER to  to get your FREE POLY TONE of the week now!  SN PoBox NR ZS subscription pw', \"* Thought I didn't see you.\", \"* Was a nice day and, impressively, i was sensible, went home early and now feel fine. Or am i just boring?! When's yours, i can't remember.\", '* Was really good to see you the other day dudette, been missing you!', '* Was thinking about chuckin ur red green n black trainners  save carryin them bac on train', '* Will be september by then!', '* Will have two more cartons off u and is very pleased with shelves', '* You gonna ring this weekend or wot?', '**FREE MESSAGE**Thanks for using the Auction Subscription Service.  . p/MSGRCVD  Skip an Auction txt OUT.  Unsubscribe txt STOP CustomerCare ', \"*deep sigh* ... I miss you :-( ... I am really surprised you haven't gone to the net cafe yet to get to me ... Don't you miss me?\", ', ,  and  picking them up from various points | going  yeovil | and they will do the motor project   hours | and then u take them home. ||   . max. || Very easy', \", how's things? Just a quick question.\", ', im .. On the snowboarding trip. I was wondering if your planning to get everyone together befor we go..a meet and greet kind of affair? Cheers, ', ', ow u dey.i paid ,thousad.i told  u would call . ', '. Tension face . Smiling face . Waste face . Innocent face .Terror face .Cruel face .Romantic face .Lovable face .decent face  &lt;#&gt; .joker face.', '. that call cost. Which i guess isnt bad. Miss ya, need ya, want ya, love ya', '. You have received your mobile content. Enjoy', '... Are you in the pub?', '....photoshop makes my computer shut down.', '.Please charge my mobile when you get up in morning.', '/ tfp', ': Kick off a new season with wks FREE goals & news to ur mobile! Txt ur club name to  eg VILLA to ', ':( but your not here....', ':-( sad puppy noise', \":-( that's not v romantic!\", ':) ', ':-) :-)', \":-) yeah! Lol. Luckily i didn't have a starring role like you!\", ';-( oh well, c u later', ';-) ok. I feel like john lennon.', '[] anyway, many good evenings to u! s', \"+ Congratulations - in this week's competition draw u have won the 贈 prize to claim just call  b. T&Cs/stop SMS . Over  only ppm\", '+ URGENT! This is the nd attempt to contact U!U have WON 贈 CALL  b  T&CsBCMWCNXX. callcost ppm mobilesvary. max贈. ', '<Forwarded from >FREE entry into our 贈 weekly comp just send the word ENTER to  NOW.  T&C www.textcomp.com', '<Forwarded from >Hi - this is your Mailbox Messaging SMS alert. You have  matches. Please call back on  to retrieve your messages and matches ccp/min', '<Forwarded from >Hi - this is your Mailbox Messaging SMS alert. You have  messages. You have  matches. Please call back on  to retrieve your messages and matches', '<Forwarded from >Please CALL  immediately as there is an urgent message waiting for you.', 'A 贈 XMAS REWARD IS WAITING FOR YOU! Our computer has randomly picked you from our loyal mobile customers to receive a 贈 reward. Just call ', 'A 贈 XMAS REWARD IS WAITING FOR YOU! Our computer has randomly picked you from our loyal mobile customers to receive a 贈 reward. Just call  ', 'A bit of Ur smile is my hppnss, a drop of Ur tear is my sorrow, a part of Ur heart is my life, a heart like mine wil care for U, forevr as my GOODFRIEND', \"A bloo bloo bloo I'll miss the first bowl\", 'A Boy loved a gal. He propsd bt she didnt mind. He gv lv lttrs, Bt her frnds threw thm. Again d boy decided  aproach d gal , dt time a truck was speeding towards d gal. Wn it was about  hit d girl,d boy ran like hell n saved her. She asked \\'hw cn u run so fast?\\' D boy replied \"Boost is d secret of my energy\" n instantly d girl shouted \"our energy\" n Thy lived happily gthr drinking boost evrydy Moral of d story:- I hv free msgs:D;): gud ni', 'A boy was late  home. His father: \"POWER OF FRNDSHIP\"', 'A cute thought for friendship: \"Its not necessary to share every secret with ur close Frnd, but watever u shared should be true\"....', \"A famous quote : when you develop the ability to listen to 'anything' unconditionally without losing your temper or self confidence, it means you are ......... 'MARRIED'\", \"A few people are at the game, I'm at the mall with iouri and kaila\", 'A gram usually runs like  &lt;#&gt; , a half eighth is smarter though and gets you almost a whole second gram for  &lt;#&gt;', 'A guy who gets used but is too dumb to realize it.', 'A link to your picture has been sent. You can also use http://alto.co.uk/wave/wave.asp?o=', \"A little. Meds say take once every  hours. It's only been  but pain is back. So I took another. Hope I don't die\", 'A lot of this sickness thing going round. Take it easy. Hope u feel better soon. Lol', 'A pure hearted person can have a wonderful smile that makes even his/her enemies to feel guilty for being an enemy.. So catch the world with your smile..:) GOODMORNING &amp; HAVE A SMILEY SUNDAY..:)', 'A swt thought: \"Nver get tired of doing little things  lovable persons..\" Coz..somtimes those little things occupy d biggest part in their Hearts.. Gud ni', 'A$NETWORKS allow companies to bill for SMS, so they are responsible for their \"suppliers\", just as a shop has to give a guarantee on what they sell. B. G.', ' and dont worry well have finished by march  ish!', ' we r stayin here an extra week, back next wed. How did we do in the rugby this weekend? Hi to and and , c u soon \"', \"Aah bless! How's your arm?\", \"Aah! A cuddle would be lush! I'd need lots of tea and soup before any kind of fumbling!\", 'Aaooooright are you at work?', 'aathi..where are you dear..', 'Abeg, make profit. But its a start. Are you using it to get sponsors for the next event?', 'About  &lt;#&gt; bucks. The banks fees are fixed. Better to call the bank and find out.', 'accordingly. I repeat, just text the word ok on your mobile phone and send', 'Actually fuck that, just do whatever, do find an excuse to be in tampa at some point before january though', 'Actually getting ready to leave the house.', \"Actually I decided I was too hungry so I haven't left yet :V\", 'Actually i deleted my old website..now i m blogging at magicalsongs.blogspot.com', \"Actually i'm waiting for  weeks when they start putting ad.\", 'Actually nvm, got hella cash, we still on for  &lt;#&gt; ish?', 'Actually, my mobile is full of msg. And i m doing a work online, where i need to send them  &lt;#&gt;  sent msg i wil explain u later.', 'Adult  Content Your video will be with you shortly', 'Aft i finish my lunch then i go str down lor. Ard  smth lor. U finish ur lunch already?', 'After completed degree. There is no use in joining finance.', 'After my work ah... Den  plus lor... U workin oso rite... Den go orchard lor, no other place to go liao...', 'After the drug she will be able to eat.', 'Ah poop. Looks like ill prob have to send in my laptop to get fixed cuz it has a gpu problem', 'AH POOR BABY!HOPE URFEELING BETTERSN LUV! PROBTHAT OVERDOSE OF WORK HEY GO CAREFUL SPK  U SN LOTS OF LOVEJEN XXX.', 'Ah you see. You have to be in the lingo. I will let you know wot on earth it is when has finished making it!', 'Ah, well that confuses things, doesnt it?', 'Ah, well that confuses things, doesnt it? I thought was friends with now. Maybe i did the wrong thing but i already sort of invited -tho he may not come cos of money.', 'Ahhh. Work. I vaguely remember that! What does it feel like? Lol', 'Ahhhh...just woken up!had a bad dream about u tho,so i dont like u right now :) i didnt know anything about comedy night but i guess im up for it.', 'Aight do you still want to get money', \"Aight fuck it, I'll get it later\", 'Aight ill get on fb in a couple minutes', \"Aight I'll grab something to eat too, text me when you're back at mu\", \"Aight I've been set free, think you could text me blake's address? It occurs to me I'm not quite as sure what I'm doing as I thought I was\", \"Aight no rush, I'll ask jay\", 'Aight should I just plan to come up later tonight?', \"Aight sorry I take ten years to shower. What's the plan?\", \"Aight text me when you're back at mu and I'll swing by, need somebody to get the door for me\", \"Aight that'll work, thanks\", 'aight we can pick some up, you open before tonight?', 'Aight well keep me informed', 'Aight what time you want me to come up?', 'Aight will do, thanks again for comin out', 'Aight yo, dats straight dogg', \"Aight, call me once you're close\", 'Aight, can you text me the address?', 'Aight, I should be there by  at the latest, probably closer to . Are jay and tyler down or should we just do two trips?', \"Aight, I'll ask a few of my roommates\", \"Aight, I'll hit you up when I get some cash\", \"Aight, I'll text you when I'm back\", \"Aight, I'm chillin in a friend's room so text me when you're on the way\", \"Aight, lemme know what's up\", \"Aight, let me know when you're gonna be around usf\", 'Aight, see you in a bit', 'Aight, sounds good. When do you want me to come down?', \"Aight, text me tonight and we'll see what's up\", 'Aight, tomorrow around  &lt;#&gt;  it is', \"Aight, we'll head out in a few\", \"Aight, you close by or still down around alex's place?\", 'Aiya we discuss later lar... Pick 端 up at  is it?', 'Aiyah e rain like quite big leh. If drizzling i can at least run home.', 'Aiyah ok wat as long as got improve can already wat...', 'Aiyah sorry lor... I watch tv watch until i forgot  check my phone.', 'Aiyah then i wait lor. Then u entertain me. Hee...', 'Aiyah u did ok already lar. E nydc at wheellock?', 'Aiyar dun disturb u liao... Thk u have lots  do aft ur cupboard come...', 'Aiyar hard  type. U later free then tell me then i call n scold n tell u.', 'Aiyar sorry lor forgot  tell u...', \"Aiyar u so poor thing... I give u my support k... Jia you! I'll think of u...\", 'Aiyo a bit pai seh 端 noe... Scared he dun rem who i am then die... Hee... But he become better lookin oredi leh...', 'Aiyo cos i sms 端 then 端 neva reply so i wait  端 to reply lar. I tot 端 havent finish ur lab wat.', 'Aiyo please 端 got time meh.', 'Aiyo u so poor thing... Then u dun wan  eat? U bathe already?', \"Aiyo... Her lesson so early... I'm still sleepin, haha... Okie, u go home liao den confirm w me lor...\", 'Aiyo... U always c our ex one... I dunno abt mei, she haven reply... First time u reply so fast... Y so lucky not workin huh, got bao by ur sugardad ah...gee.. ', 'Al he does is moan at me if n e thin goes wrong its my fault&al de arguments r my fault&fed up of him of himso y bother? Hav go, thanx.xx']\n"
     ]
    }
   ],
   "source": [
    "print (\"The list after removing duplicates : \" + str(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages\n",
    "\n",
    "## TfidfVectorizer - Transforms text to feature vectors that can be used as input to estimator. \n",
    "##  Cosine_Similarity - \n",
    "### Cosine similarity is a metric used to determine how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space.Smaller the angle, higher the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation of the list to feature vectors using Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "trans = tf_idf.fit_transform(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the cosine_similarity to the transformed vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = cosine_similarity(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.02936368 ... 0.         0.094808   0.        ]\n",
      " [0.         1.         0.24811677 ... 0.         0.04857442 0.        ]\n",
      " [0.02936368 0.24811677 1.         ... 0.         0.06336432 0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.08284928 0.04627805]\n",
      " [0.094808   0.04857442 0.06336432 ... 0.08284928 1.         0.        ]\n",
      " [0.         0.         0.         ... 0.04627805 0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000007"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Maximum value\n",
    "\n",
    "cosine_similarity.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Minimum value\n",
    "\n",
    "cosine_similarity.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Top 10 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity[np.diag_indices_from(cosine_similarity)] = 0.0 \n",
    "idx = np.argsort(cosine_similarity, axis=None)[-10:] \n",
    "midx = np.unravel_index(idx, cosine_similarity.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60684529 0.60684529 0.67086621 0.67086621 0.91100111 0.91100111\n",
      " 0.96655821 0.96655821 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "### cosine_similarity of the 10 documents\n",
    "\n",
    "print (cosine_similarity[midx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7, 69, 16, 15, 93, 94, 29, 30, 96, 97], dtype=int64),\n",
       " array([69,  7, 15, 16, 94, 93, 30, 29, 97, 96], dtype=int64))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion :\n",
    "## The top 5 pairs of similar documents are \n",
    "### pair1-(96,97)         \n",
    "### pair2-(29,30)                   \n",
    "### pair-3(93,94)          \n",
    "### pair4(15,16)\n",
    "### pair5-(7,69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
